name: "PHI3 Mini Alpaca finetuning pipeline"
pipelines:
  - name: Alpaca finetuning
    inputs: [0]
    outputs: [2]
    loadgen:
      type: poisson
      max_queries: 2000
      is_training: True
      timeout: 20000
      config:
        rate: 8 # average #requests/sec
    stages:
      - name: Alpaca dataset
        id: 0
        outputs: [1]
        component: stages.lora_finetune.Dataset
        config:
          dataset:
            component: torchtune.datasets.alpaca_cleaned_dataset
          tokenizer: 
            component: from torchtune.models.phi3.phi3_mini_tokenizer
            path: phi3-mini-4k/tokenizer.model
      - name: Alpaca dataloader
        id: 1
        inputs: [0]
        outputs: [2]
        component: stages.lora_finetune.TorchTuneDataLoader
        config:
          batch_size: 16
          shuffle: True
          split: "train"
      - name: Alpaca finetune
        id: 2
        inputs: [1]
        component: stages.lora_finetune.Finetune
        config: 
          model:
            component_: torchtune.models.phi3.qlora_phi3_mini
            lora_attn_modules: ['q_proj', 'v_proj', 'k_proj', 'output_proj']
            apply_lora_to_mlp: True
            apply_lora_to_output: False
            lora_rank: 8
            lora_alpha: 16
          checkpointer:
            component_: torchtune.utils.FullModelHFCheckpointer
            checkpoint_dir: phi3-mini-4k
            checkpoint_files: [
              model-00001-of-00002.safetensors,
              model-00002-of-00002.safetensors
            ]
            output_dir: phi3-mini-4k
            model_type: PHI3_MINI
          optimizer:
            component_: torch.optim.AdamW
            weight_decay: 0.01
            lr: 3e-4
            gradient_accumulation_steps: 16
          lr_scheduler:
            component_: torchtune.modules.get_cosine_schedule_with_warmup
            num_warmup_steps: 100
          loss:
            component_: torch.nn.CrossEntropyLoss